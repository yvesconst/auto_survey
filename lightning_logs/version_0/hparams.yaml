adam_epsilon: 1.0e-08
data_dir: ./dataset
early_stop_callback: false
eval_batch_size: 8
fp_16: false
gradient_accumulation_steps: 8
learning_rate: 0.0003
max_grad_norm: 1.0
max_seq_length: 220
model_name_or_path: google/mt5-base
n_gpu: 1
num_train_epochs: 10
opt_level: O1
output_dir: ./working/result
seed: 42
tokenizer_name_or_path: google/mt5-base
train_batch_size: 8
warmup_steps: 0
weight_decay: 0.0
