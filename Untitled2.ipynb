{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (3.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.20.3)\r\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.3.5)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.6.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\r\n",
      "Requirement already satisfied: six in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 -> batch: 20 -> loss: 1.0834256410598755\n",
      "epoch: 2 -> batch: 20 -> loss: 1.0751036405563354\n",
      "epoch: 3 -> batch: 20 -> loss: 1.0587968826293945\n",
      "epoch: 4 -> batch: 20 -> loss: 1.0361889600753784\n",
      "epoch: 5 -> batch: 20 -> loss: 1.008805513381958\n",
      "epoch: 6 -> batch: 20 -> loss: 0.9780272245407104\n",
      "epoch: 7 -> batch: 20 -> loss: 0.945094883441925\n",
      "epoch: 8 -> batch: 20 -> loss: 0.9111039042472839\n",
      "epoch: 9 -> batch: 20 -> loss: 0.8769862055778503\n",
      "epoch: 10 -> batch: 20 -> loss: 0.8434896469116211\n",
      "epoch: 11 -> batch: 20 -> loss: 0.8111866116523743\n",
      "epoch: 12 -> batch: 20 -> loss: 0.7804023027420044\n",
      "epoch: 13 -> batch: 20 -> loss: 0.7513031959533691\n",
      "epoch: 14 -> batch: 20 -> loss: 0.7239199876785278\n",
      "epoch: 15 -> batch: 20 -> loss: 0.6981942653656006\n",
      "epoch: 16 -> batch: 20 -> loss: 0.6740029454231262\n",
      "epoch: 17 -> batch: 20 -> loss: 0.6512179374694824\n",
      "epoch: 18 -> batch: 20 -> loss: 0.6296919584274292\n",
      "epoch: 19 -> batch: 20 -> loss: 0.6093002557754517\n",
      "epoch: 20 -> batch: 20 -> loss: 0.5899336338043213\n",
      "epoch: 21 -> batch: 20 -> loss: 0.5714978575706482\n",
      "epoch: 22 -> batch: 20 -> loss: 0.5539155006408691\n",
      "epoch: 23 -> batch: 20 -> loss: 0.5371218919754028\n",
      "epoch: 24 -> batch: 20 -> loss: 0.5210622549057007\n",
      "epoch: 25 -> batch: 20 -> loss: 0.505690336227417\n",
      "epoch: 26 -> batch: 20 -> loss: 0.4910117983818054\n",
      "epoch: 27 -> batch: 20 -> loss: 0.4769875407218933\n",
      "epoch: 28 -> batch: 20 -> loss: 0.4635137915611267\n",
      "epoch: 29 -> batch: 20 -> loss: 0.450585275888443\n",
      "epoch: 30 -> batch: 20 -> loss: 0.43817299604415894\n",
      "epoch: 31 -> batch: 20 -> loss: 0.42625418305397034\n",
      "epoch: 32 -> batch: 20 -> loss: 0.41481003165245056\n",
      "epoch: 33 -> batch: 20 -> loss: 0.40382543206214905\n",
      "epoch: 34 -> batch: 20 -> loss: 0.3932746350765228\n",
      "epoch: 35 -> batch: 20 -> loss: 0.38313907384872437\n",
      "epoch: 36 -> batch: 20 -> loss: 0.3734007477760315\n",
      "epoch: 37 -> batch: 20 -> loss: 0.3640840947628021\n",
      "epoch: 38 -> batch: 20 -> loss: 0.35515135526657104\n",
      "epoch: 39 -> batch: 20 -> loss: 0.3465648889541626\n",
      "epoch: 40 -> batch: 20 -> loss: 0.33830925822257996\n",
      "epoch: 41 -> batch: 20 -> loss: 0.33036965131759644\n",
      "epoch: 42 -> batch: 20 -> loss: 0.3227331042289734\n",
      "epoch: 43 -> batch: 20 -> loss: 0.3153848350048065\n",
      "epoch: 44 -> batch: 20 -> loss: 0.3083112835884094\n",
      "epoch: 45 -> batch: 20 -> loss: 0.30149969458580017\n",
      "epoch: 46 -> batch: 20 -> loss: 0.29493802785873413\n",
      "epoch: 47 -> batch: 20 -> loss: 0.2886269688606262\n",
      "epoch: 48 -> batch: 20 -> loss: 0.2840673327445984\n",
      "epoch: 49 -> batch: 20 -> loss: 0.28041112422943115\n",
      "epoch: 50 -> batch: 20 -> loss: 0.2768540382385254\n",
      "epoch: 1 -> batch: 40 -> loss: 0.07923508435487747\n",
      "epoch: 2 -> batch: 40 -> loss: 0.07843442261219025\n",
      "epoch: 3 -> batch: 40 -> loss: 0.0775211751461029\n",
      "epoch: 4 -> batch: 40 -> loss: 0.07651285827159882\n",
      "epoch: 5 -> batch: 40 -> loss: 0.07542592287063599\n",
      "epoch: 6 -> batch: 40 -> loss: 0.07427551597356796\n",
      "epoch: 7 -> batch: 40 -> loss: 0.07307557761669159\n",
      "epoch: 8 -> batch: 40 -> loss: 0.07183893024921417\n",
      "epoch: 9 -> batch: 40 -> loss: 0.07057757675647736\n",
      "epoch: 10 -> batch: 40 -> loss: 0.06930053234100342\n",
      "epoch: 11 -> batch: 40 -> loss: 0.06801661849021912\n",
      "epoch: 12 -> batch: 40 -> loss: 0.06673340499401093\n",
      "epoch: 13 -> batch: 40 -> loss: 0.06545739620923996\n",
      "epoch: 14 -> batch: 40 -> loss: 0.06419411301612854\n",
      "epoch: 15 -> batch: 40 -> loss: 0.06294817477464676\n",
      "epoch: 16 -> batch: 40 -> loss: 0.06172337010502815\n",
      "epoch: 17 -> batch: 40 -> loss: 0.06052275747060776\n",
      "epoch: 18 -> batch: 40 -> loss: 0.05934872105717659\n",
      "epoch: 19 -> batch: 40 -> loss: 0.05820303410291672\n",
      "epoch: 20 -> batch: 40 -> loss: 0.05708710104227066\n",
      "epoch: 21 -> batch: 40 -> loss: 0.05600181221961975\n",
      "epoch: 22 -> batch: 40 -> loss: 0.05494759604334831\n",
      "epoch: 23 -> batch: 40 -> loss: 0.053924739360809326\n",
      "epoch: 24 -> batch: 40 -> loss: 0.05293319374322891\n",
      "epoch: 25 -> batch: 40 -> loss: 0.0519726499915123\n",
      "epoch: 26 -> batch: 40 -> loss: 0.05104265734553337\n",
      "epoch: 27 -> batch: 40 -> loss: 0.05014266446232796\n",
      "epoch: 28 -> batch: 40 -> loss: 0.049359943717718124\n",
      "epoch: 29 -> batch: 40 -> loss: 0.04861471801996231\n",
      "epoch: 30 -> batch: 40 -> loss: 0.047891147434711456\n",
      "epoch: 31 -> batch: 40 -> loss: 0.04718834161758423\n",
      "epoch: 32 -> batch: 40 -> loss: 0.04650552198290825\n",
      "epoch: 33 -> batch: 40 -> loss: 0.045841850340366364\n",
      "epoch: 34 -> batch: 40 -> loss: 0.0451965369284153\n",
      "epoch: 35 -> batch: 40 -> loss: 0.044568877667188644\n",
      "epoch: 36 -> batch: 40 -> loss: 0.04395813122391701\n",
      "epoch: 37 -> batch: 40 -> loss: 0.043363649398088455\n",
      "epoch: 38 -> batch: 40 -> loss: 0.04278476908802986\n",
      "epoch: 39 -> batch: 40 -> loss: 0.042220912873744965\n",
      "epoch: 40 -> batch: 40 -> loss: 0.041671477258205414\n",
      "epoch: 41 -> batch: 40 -> loss: 0.04113592952489853\n",
      "epoch: 42 -> batch: 40 -> loss: 0.04061368852853775\n",
      "epoch: 43 -> batch: 40 -> loss: 0.04010431095957756\n",
      "epoch: 44 -> batch: 40 -> loss: 0.03960726410150528\n",
      "epoch: 45 -> batch: 40 -> loss: 0.03912211209535599\n",
      "epoch: 46 -> batch: 40 -> loss: 0.03864845633506775\n",
      "epoch: 47 -> batch: 40 -> loss: 0.03818577527999878\n",
      "epoch: 48 -> batch: 40 -> loss: 0.0377337746322155\n",
      "epoch: 49 -> batch: 40 -> loss: 0.03729204088449478\n",
      "epoch: 50 -> batch: 40 -> loss: 0.03686022758483887\n",
      "epoch: 1 -> batch: 60 -> loss: 0.020475028082728386\n",
      "epoch: 2 -> batch: 60 -> loss: 0.020173024386167526\n",
      "epoch: 3 -> batch: 60 -> loss: 0.0198492631316185\n",
      "epoch: 4 -> batch: 60 -> loss: 0.019508546218276024\n",
      "epoch: 5 -> batch: 60 -> loss: 0.01915488764643669\n",
      "epoch: 6 -> batch: 60 -> loss: 0.018792064860463142\n",
      "epoch: 7 -> batch: 60 -> loss: 0.01842336542904377\n",
      "epoch: 8 -> batch: 60 -> loss: 0.018051637336611748\n",
      "epoch: 9 -> batch: 60 -> loss: 0.017679288983345032\n",
      "epoch: 10 -> batch: 60 -> loss: 0.01731290854513645\n",
      "epoch: 11 -> batch: 60 -> loss: 0.01695312187075615\n",
      "epoch: 12 -> batch: 60 -> loss: 0.016598431393504143\n",
      "epoch: 13 -> batch: 60 -> loss: 0.016249965876340866\n",
      "epoch: 14 -> batch: 60 -> loss: 0.01590854860842228\n",
      "epoch: 15 -> batch: 60 -> loss: 0.015574847348034382\n",
      "epoch: 16 -> batch: 60 -> loss: 0.015249339863657951\n",
      "epoch: 17 -> batch: 60 -> loss: 0.014932421036064625\n",
      "epoch: 18 -> batch: 60 -> loss: 0.014624306932091713\n",
      "epoch: 19 -> batch: 60 -> loss: 0.014325174503028393\n",
      "epoch: 20 -> batch: 60 -> loss: 0.014035074040293694\n",
      "epoch: 21 -> batch: 60 -> loss: 0.013753995299339294\n",
      "epoch: 22 -> batch: 60 -> loss: 0.013481883332133293\n",
      "epoch: 23 -> batch: 60 -> loss: 0.013218628242611885\n",
      "epoch: 24 -> batch: 60 -> loss: 0.012964097782969475\n",
      "epoch: 25 -> batch: 60 -> loss: 0.012718024663627148\n",
      "epoch: 26 -> batch: 60 -> loss: 0.012480292469263077\n",
      "epoch: 27 -> batch: 60 -> loss: 0.01225065253674984\n",
      "epoch: 28 -> batch: 60 -> loss: 0.012028805911540985\n",
      "epoch: 29 -> batch: 60 -> loss: 0.01181455422192812\n",
      "epoch: 30 -> batch: 60 -> loss: 0.011607696302235126\n",
      "epoch: 31 -> batch: 60 -> loss: 0.011407934129238129\n",
      "epoch: 32 -> batch: 60 -> loss: 0.011214985512197018\n",
      "epoch: 33 -> batch: 60 -> loss: 0.011028651148080826\n",
      "epoch: 34 -> batch: 60 -> loss: 0.010848591104149818\n",
      "epoch: 35 -> batch: 60 -> loss: 0.010674610733985901\n",
      "epoch: 36 -> batch: 60 -> loss: 0.010506460443139076\n",
      "epoch: 37 -> batch: 60 -> loss: 0.010343866422772408\n",
      "epoch: 38 -> batch: 60 -> loss: 0.010186655446887016\n",
      "epoch: 39 -> batch: 60 -> loss: 0.010034547187387943\n",
      "epoch: 40 -> batch: 60 -> loss: 0.00988740473985672\n",
      "epoch: 41 -> batch: 60 -> loss: 0.009744951501488686\n",
      "epoch: 42 -> batch: 60 -> loss: 0.009607018902897835\n",
      "epoch: 43 -> batch: 60 -> loss: 0.009473429061472416\n",
      "epoch: 44 -> batch: 60 -> loss: 0.009343949146568775\n",
      "epoch: 45 -> batch: 60 -> loss: 0.009218460880219936\n",
      "epoch: 46 -> batch: 60 -> loss: 0.00909675657749176\n",
      "epoch: 47 -> batch: 60 -> loss: 0.008978706784546375\n",
      "epoch: 48 -> batch: 60 -> loss: 0.008864144794642925\n",
      "epoch: 49 -> batch: 60 -> loss: 0.008752921596169472\n",
      "epoch: 50 -> batch: 60 -> loss: 0.00864492729306221\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as T\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "device = \"cuda\" if T.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class ClassificationNet(T.nn.Module):\n",
    "  def __init__(self, in_features=6, out_features=3, n_per_layer=10):\n",
    "    super(ClassificationNet, self).__init__()\n",
    "    self.fc1 = nn.Linear(in_features, n_per_layer) \n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(n_per_layer, n_per_layer) \n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(n_per_layer, out_features) \n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu1(out)\n",
    "    out = self.fc2(out)\n",
    "    out = self.relu2(out)\n",
    "    out = self.fc3(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def next_batch(inputs, targets, batchSize):\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "         yield (inputs[i:i + batchSize], targets[i:i + batchSize])\n",
    "\n",
    "df_student = pd.read_csv(\"./dataset/merged_student_engagement_level.csv\")\n",
    "\n",
    "df_experiment =  df_student.copy()\n",
    "\n",
    "\n",
    "df_experiment['engagement'] = df_experiment['engagement'].map(\n",
    "    {'H': 2, 'M': 1, 'L': 0}\n",
    ")\n",
    "\n",
    "X = df_experiment.loc[:,[\"reviews\", \"a1\", \"a2\", \"a3\", \"gender\", \"grade\"]].values\n",
    "y = df_experiment.iloc[:, 4].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# For low ressouces\n",
    "X_train = X_train[0:700]\n",
    "y_train = y_train[0:700]\n",
    "\n",
    "X_test = T.FloatTensor(X_test)\n",
    "y_train = T.LongTensor(y_train)\n",
    "y_test = T.LongTensor(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = T.from_numpy(scaler.fit_transform(X_train)).float()\n",
    "X_test = T.from_numpy(scaler.fit_transform(X_test)).float()\n",
    "\n",
    "epochs = 50\n",
    "BATCH_SIZE = 10\n",
    "losses = []\n",
    "model = ClassificationNet().to(device)\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = T.nn.CrossEntropyLoss()\n",
    "j = 0\n",
    "\n",
    "for (batchX, batchY) in next_batch(X_train, y_train, BATCH_SIZE):\n",
    "    (batchX, batchY) = (batchX, batchY.to(device))\n",
    "    j += 1\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        i += 1\n",
    "        \n",
    "        y_pred = model(batchX)\n",
    "        loss = criterion(y_pred, batchY.long())\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if j % 20 == 0:\n",
    "            print(f'epoch: {i} -> batch: {j} -> loss: {loss}')\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        T.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "with T.no_grad():\n",
    "    model.eval()\n",
    "    pred = model(X_test)\n",
    "    _, pr = T.max(pred, 1)\n",
    "    \n",
    "    '''\n",
    "        Engagement level \n",
    "        'High': 2, 'Medium': 1, 'Low': 0\n",
    "    '''\n",
    "    conf_mat = confusion_matrix(pr.tolist(), y_test.tolist())\n",
    "    sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "\n",
    "T.save(model, \"./models/PriorProfile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
