{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.6.2)\r\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.3.5)\r\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (3.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.20.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\r\n",
      "Requirement already satisfied: six in /Users/ytadjota/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 -> batch: 20 -> loss: 1.3460524082183838\n",
      "epoch: 2 -> batch: 20 -> loss: 1.3335330486297607\n",
      "epoch: 3 -> batch: 20 -> loss: 1.3103610277175903\n",
      "epoch: 4 -> batch: 20 -> loss: 1.279278039932251\n",
      "epoch: 5 -> batch: 20 -> loss: 1.2427157163619995\n",
      "epoch: 6 -> batch: 20 -> loss: 1.2027432918548584\n",
      "epoch: 7 -> batch: 20 -> loss: 1.161049723625183\n",
      "epoch: 8 -> batch: 20 -> loss: 1.1189568042755127\n",
      "epoch: 9 -> batch: 20 -> loss: 1.0774507522583008\n",
      "epoch: 10 -> batch: 20 -> loss: 1.03731369972229\n",
      "epoch: 11 -> batch: 20 -> loss: 0.9993802309036255\n",
      "epoch: 12 -> batch: 20 -> loss: 0.9632631540298462\n",
      "epoch: 13 -> batch: 20 -> loss: 0.9290496110916138\n",
      "epoch: 14 -> batch: 20 -> loss: 0.8967069387435913\n",
      "epoch: 15 -> batch: 20 -> loss: 0.8660290837287903\n",
      "epoch: 16 -> batch: 20 -> loss: 0.8370046615600586\n",
      "epoch: 17 -> batch: 20 -> loss: 0.8094987869262695\n",
      "epoch: 18 -> batch: 20 -> loss: 0.7833712100982666\n",
      "epoch: 19 -> batch: 20 -> loss: 0.7584953904151917\n",
      "epoch: 20 -> batch: 20 -> loss: 0.7347597479820251\n",
      "epoch: 21 -> batch: 20 -> loss: 0.7120550870895386\n",
      "epoch: 22 -> batch: 20 -> loss: 0.6902939081192017\n",
      "epoch: 23 -> batch: 20 -> loss: 0.6694250106811523\n",
      "epoch: 24 -> batch: 20 -> loss: 0.6493731737136841\n",
      "epoch: 25 -> batch: 20 -> loss: 0.6300808191299438\n",
      "epoch: 26 -> batch: 20 -> loss: 0.6121851801872253\n",
      "epoch: 27 -> batch: 20 -> loss: 0.5950958728790283\n",
      "epoch: 28 -> batch: 20 -> loss: 0.5786136388778687\n",
      "epoch: 29 -> batch: 20 -> loss: 0.5627166628837585\n",
      "epoch: 30 -> batch: 20 -> loss: 0.5474892258644104\n",
      "epoch: 31 -> batch: 20 -> loss: 0.5329732894897461\n",
      "epoch: 32 -> batch: 20 -> loss: 0.5187883973121643\n",
      "epoch: 33 -> batch: 20 -> loss: 0.5048399567604065\n",
      "epoch: 34 -> batch: 20 -> loss: 0.49132782220840454\n",
      "epoch: 35 -> batch: 20 -> loss: 0.47829142212867737\n",
      "epoch: 36 -> batch: 20 -> loss: 0.46567338705062866\n",
      "epoch: 37 -> batch: 20 -> loss: 0.45347127318382263\n",
      "epoch: 38 -> batch: 20 -> loss: 0.4417427182197571\n",
      "epoch: 39 -> batch: 20 -> loss: 0.43043842911720276\n",
      "epoch: 40 -> batch: 20 -> loss: 0.4196031987667084\n",
      "epoch: 41 -> batch: 20 -> loss: 0.40918636322021484\n",
      "epoch: 42 -> batch: 20 -> loss: 0.3991501033306122\n",
      "epoch: 43 -> batch: 20 -> loss: 0.38948509097099304\n",
      "epoch: 44 -> batch: 20 -> loss: 0.38018137216567993\n",
      "epoch: 45 -> batch: 20 -> loss: 0.3712286353111267\n",
      "epoch: 46 -> batch: 20 -> loss: 0.362616628408432\n",
      "epoch: 47 -> batch: 20 -> loss: 0.3543348014354706\n",
      "epoch: 48 -> batch: 20 -> loss: 0.3463757634162903\n",
      "epoch: 49 -> batch: 20 -> loss: 0.33873388171195984\n",
      "epoch: 50 -> batch: 20 -> loss: 0.3314978778362274\n",
      "epoch: 1 -> batch: 40 -> loss: 0.08989643305540085\n",
      "epoch: 2 -> batch: 40 -> loss: 0.08904669433832169\n",
      "epoch: 3 -> batch: 40 -> loss: 0.08804579079151154\n",
      "epoch: 4 -> batch: 40 -> loss: 0.08691462874412537\n",
      "epoch: 5 -> batch: 40 -> loss: 0.08567291498184204\n",
      "epoch: 6 -> batch: 40 -> loss: 0.08433909714221954\n",
      "epoch: 7 -> batch: 40 -> loss: 0.08293010294437408\n",
      "epoch: 8 -> batch: 40 -> loss: 0.08146153390407562\n",
      "epoch: 9 -> batch: 40 -> loss: 0.07995321601629257\n",
      "epoch: 10 -> batch: 40 -> loss: 0.0784168615937233\n",
      "epoch: 11 -> batch: 40 -> loss: 0.0768587738275528\n",
      "epoch: 12 -> batch: 40 -> loss: 0.07528899610042572\n",
      "epoch: 13 -> batch: 40 -> loss: 0.07371625304222107\n",
      "epoch: 14 -> batch: 40 -> loss: 0.07214822620153427\n",
      "epoch: 15 -> batch: 40 -> loss: 0.0705915316939354\n",
      "epoch: 16 -> batch: 40 -> loss: 0.06905170530080795\n",
      "epoch: 17 -> batch: 40 -> loss: 0.06753353774547577\n",
      "epoch: 18 -> batch: 40 -> loss: 0.06604088842868805\n",
      "epoch: 19 -> batch: 40 -> loss: 0.06457705050706863\n",
      "epoch: 20 -> batch: 40 -> loss: 0.06319545209407806\n",
      "epoch: 21 -> batch: 40 -> loss: 0.061902452260255814\n",
      "epoch: 22 -> batch: 40 -> loss: 0.060641784220933914\n",
      "epoch: 23 -> batch: 40 -> loss: 0.059413861483335495\n",
      "epoch: 24 -> batch: 40 -> loss: 0.058197248727083206\n",
      "epoch: 25 -> batch: 40 -> loss: 0.05697976425290108\n",
      "epoch: 26 -> batch: 40 -> loss: 0.05578844994306564\n",
      "epoch: 27 -> batch: 40 -> loss: 0.05462484806776047\n",
      "epoch: 28 -> batch: 40 -> loss: 0.05355207994580269\n",
      "epoch: 29 -> batch: 40 -> loss: 0.0525398850440979\n",
      "epoch: 30 -> batch: 40 -> loss: 0.0515560507774353\n",
      "epoch: 31 -> batch: 40 -> loss: 0.05060005187988281\n",
      "epoch: 32 -> batch: 40 -> loss: 0.04967132583260536\n",
      "epoch: 33 -> batch: 40 -> loss: 0.048769257962703705\n",
      "epoch: 34 -> batch: 40 -> loss: 0.04789320006966591\n",
      "epoch: 35 -> batch: 40 -> loss: 0.04704243317246437\n",
      "epoch: 36 -> batch: 40 -> loss: 0.04623206704854965\n",
      "epoch: 37 -> batch: 40 -> loss: 0.045475639402866364\n",
      "epoch: 38 -> batch: 40 -> loss: 0.04474011808633804\n",
      "epoch: 39 -> batch: 40 -> loss: 0.04402482137084007\n",
      "epoch: 40 -> batch: 40 -> loss: 0.04332917556166649\n",
      "epoch: 41 -> batch: 40 -> loss: 0.04265246167778969\n",
      "epoch: 42 -> batch: 40 -> loss: 0.0419941172003746\n",
      "epoch: 43 -> batch: 40 -> loss: 0.04135355353355408\n",
      "epoch: 44 -> batch: 40 -> loss: 0.0407300740480423\n",
      "epoch: 45 -> batch: 40 -> loss: 0.040123194456100464\n",
      "epoch: 46 -> batch: 40 -> loss: 0.039532337337732315\n",
      "epoch: 47 -> batch: 40 -> loss: 0.03895697742700577\n",
      "epoch: 48 -> batch: 40 -> loss: 0.03839651495218277\n",
      "epoch: 49 -> batch: 40 -> loss: 0.037850506603717804\n",
      "epoch: 50 -> batch: 40 -> loss: 0.03731846809387207\n",
      "epoch: 1 -> batch: 60 -> loss: 0.012038243003189564\n",
      "epoch: 2 -> batch: 60 -> loss: 0.011876572854816914\n",
      "epoch: 3 -> batch: 60 -> loss: 0.011720349080860615\n",
      "epoch: 4 -> batch: 60 -> loss: 0.011568507179617882\n",
      "epoch: 5 -> batch: 60 -> loss: 0.011418643407523632\n",
      "epoch: 6 -> batch: 60 -> loss: 0.01127095427364111\n",
      "epoch: 7 -> batch: 60 -> loss: 0.011125572957098484\n",
      "epoch: 8 -> batch: 60 -> loss: 0.01098258700221777\n",
      "epoch: 9 -> batch: 60 -> loss: 0.01084214262664318\n",
      "epoch: 10 -> batch: 60 -> loss: 0.010704349726438522\n",
      "epoch: 11 -> batch: 60 -> loss: 0.010569309815764427\n",
      "epoch: 12 -> batch: 60 -> loss: 0.010437010787427425\n",
      "epoch: 13 -> batch: 60 -> loss: 0.010307520627975464\n",
      "epoch: 14 -> batch: 60 -> loss: 0.010180818848311901\n",
      "epoch: 15 -> batch: 60 -> loss: 0.010056992992758751\n",
      "epoch: 16 -> batch: 60 -> loss: 0.009936319664120674\n",
      "epoch: 17 -> batch: 60 -> loss: 0.009818436577916145\n",
      "epoch: 18 -> batch: 60 -> loss: 0.009703302755951881\n",
      "epoch: 19 -> batch: 60 -> loss: 0.009590942412614822\n",
      "epoch: 20 -> batch: 60 -> loss: 0.009481290355324745\n",
      "epoch: 21 -> batch: 60 -> loss: 0.009374234825372696\n",
      "epoch: 22 -> batch: 60 -> loss: 0.009269790723919868\n",
      "epoch: 23 -> batch: 60 -> loss: 0.00916781835258007\n",
      "epoch: 24 -> batch: 60 -> loss: 0.009068324230611324\n",
      "epoch: 25 -> batch: 60 -> loss: 0.008971262723207474\n",
      "epoch: 26 -> batch: 60 -> loss: 0.00887653324753046\n",
      "epoch: 27 -> batch: 60 -> loss: 0.008784006349742413\n",
      "epoch: 28 -> batch: 60 -> loss: 0.008693734183907509\n",
      "epoch: 29 -> batch: 60 -> loss: 0.008605613373219967\n",
      "epoch: 30 -> batch: 60 -> loss: 0.0085195517167449\n",
      "epoch: 31 -> batch: 60 -> loss: 0.00843549519777298\n",
      "epoch: 32 -> batch: 60 -> loss: 0.008353407494723797\n",
      "epoch: 33 -> batch: 60 -> loss: 0.008273222483694553\n",
      "epoch: 34 -> batch: 60 -> loss: 0.008194846101105213\n",
      "epoch: 35 -> batch: 60 -> loss: 0.008118247613310814\n",
      "epoch: 36 -> batch: 60 -> loss: 0.00804336741566658\n",
      "epoch: 37 -> batch: 60 -> loss: 0.00797016266733408\n",
      "epoch: 38 -> batch: 60 -> loss: 0.007898537442088127\n",
      "epoch: 39 -> batch: 60 -> loss: 0.00782846286892891\n",
      "epoch: 40 -> batch: 60 -> loss: 0.007762023713439703\n",
      "epoch: 41 -> batch: 60 -> loss: 0.00769814383238554\n",
      "epoch: 42 -> batch: 60 -> loss: 0.007635605521500111\n",
      "epoch: 43 -> batch: 60 -> loss: 0.007574363611638546\n",
      "epoch: 44 -> batch: 60 -> loss: 0.007514333818107843\n",
      "epoch: 45 -> batch: 60 -> loss: 0.007455590181052685\n",
      "epoch: 46 -> batch: 60 -> loss: 0.007398942019790411\n",
      "epoch: 47 -> batch: 60 -> loss: 0.00734346266835928\n",
      "epoch: 48 -> batch: 60 -> loss: 0.007289013359695673\n",
      "epoch: 49 -> batch: 60 -> loss: 0.007235628552734852\n",
      "epoch: 50 -> batch: 60 -> loss: 0.007183205336332321\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from prior_profile import ClassificationNet\n",
    "\n",
    "device = \"cuda\" if T.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def next_batch(inputs, targets, batchSize):\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "         yield (inputs[i:i + batchSize], targets[i:i + batchSize])\n",
    "\n",
    "df_student = pd.read_csv(\"./dataset/merged_student_engagement_level.csv\")\n",
    "\n",
    "df_experiment =  df_student.copy()\n",
    "\n",
    "\n",
    "df_experiment['engagement'] = df_experiment['engagement'].map(\n",
    "    {'H': 2, 'M': 1, 'L': 0}\n",
    ")\n",
    "\n",
    "X = df_experiment.loc[:,[\"reviews\", \"a1\", \"a2\", \"a3\", \"gender\", \"grade\"]].values\n",
    "y = df_experiment.iloc[:, 4].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# For low ressouces\n",
    "X_train = X_train[0:700]\n",
    "y_train = y_train[0:700]\n",
    "\n",
    "X_test = T.FloatTensor(X_test)\n",
    "y_train = T.LongTensor(y_train)\n",
    "y_test = T.LongTensor(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = T.from_numpy(scaler.fit_transform(X_train)).float()\n",
    "X_test = T.from_numpy(scaler.fit_transform(X_test)).float()\n",
    "\n",
    "epochs = 50\n",
    "BATCH_SIZE = 10\n",
    "losses = []\n",
    "model = ClassificationNet().to(device)\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = T.nn.CrossEntropyLoss()\n",
    "j = 0\n",
    "\n",
    "for (batchX, batchY) in next_batch(X_train, y_train, BATCH_SIZE):\n",
    "    (batchX, batchY) = (batchX, batchY.to(device))\n",
    "    j += 1\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        i += 1\n",
    "        \n",
    "        y_pred = model(batchX)\n",
    "        loss = criterion(y_pred, batchY.long())\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if j % 20 == 0:\n",
    "            print(f'epoch: {i} -> batch: {j} -> loss: {loss}')\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        T.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with T.no_grad():\n",
    "    model.eval()\n",
    "    pred = model(X_test)\n",
    "    _, pr = T.max(pred, 1)\n",
    "    \n",
    "    '''\n",
    "        Engagement level \n",
    "        'High': 2, 'Medium': 1, 'Low': 0\n",
    "    '''\n",
    "    conf_mat = confusion_matrix(pr.tolist(), y_test.tolist())\n",
    "    sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "\n",
    "T.save(model, \"./models/PriorProfile\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
